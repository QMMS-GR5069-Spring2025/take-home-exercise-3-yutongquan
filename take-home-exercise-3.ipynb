{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051a2b24-04ac-4041-a935-866a8907c381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029f2645-08ab-4ea6-a12b-1db36a216a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/Users/yq2397@columbia.edu/take-home-exercise-3-yq2397\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c86af4c-096e-435f-b427-f062fc26e309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b607de-dac7-4709-a350-0921225353fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header = True)\n",
    "df_races= spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2a6dd5-f3f7-4e26-8168-5c7b67cb269f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df = df_results.join(df_races, on=\"raceId\", how=\"left\")\n",
    "merged_df = merged_df.join(df_drivers, on=\"driverId\", how=\"left\")\n",
    "display(merged_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a462ee11-e9d5-4fea-b2de-00c853e1a744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merged_df.withColumn(\"grid\", merged_df[\"grid\"].cast(DoubleType()))\n",
    "merged_df = merged_df.withColumn(\"positionOrder\", merged_df[\"positionOrder\"].cast(DoubleType()))\n",
    "merged_df = merged_df.withColumn(\"laps\", merged_df[\"laps\"].cast(DoubleType()))\n",
    "\n",
    "merged_df = merged_df.withColumn(\"dob\", F.to_date(merged_df[\"dob\"]))\n",
    "merged_df = merged_df.withColumn(\"date\", F.to_date(merged_df[\"date\"]))\n",
    "merged_df = merged_df.withColumn(\"driver_age\", \n",
    "                                F.datediff(merged_df[\"date\"], merged_df[\"dob\"])/365.25)\n",
    "\n",
    "model_df = merged_df.select(\"grid\", \"driver_age\", \"laps\", \"positionOrder\")\n",
    "model_df = model_df.dropna()\n",
    "\n",
    "print(f\"Total records for modeling: {model_df.count()}\")\n",
    "display(model_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f04d69-6322-45bc-9daf-61d2d249571a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set count: {train_df.count()}\")\n",
    "print(f\"Testing set count: {test_df.count()}\")\n",
    "\n",
    "feature_cols = [\"grid\", \"driver_age\", \"laps\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e60b0d-f7b7-4c54-aa8b-e8a002ddd83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model_type, params=None):\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        train_vector = assembler.transform(train_df)\n",
    "        test_vector = assembler.transform(test_df)\n",
    "\n",
    "        if model_type == 'rf':\n",
    "            model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"positionOrder\", **params if params else {})\n",
    "        elif model_type == 'gbt':\n",
    "            model = GBTRegressor(featuresCol=\"features\", labelCol=\"positionOrder\", **params if params else {})\n",
    "\n",
    "        trained_model = model.fit(train_vector)\n",
    "\n",
    "        predictions = trained_model.transform(test_vector)\n",
    "\n",
    "        evaluator = RegressionEvaluator(labelCol=\"positionOrder\", predictionCol=\"prediction\")\n",
    "        rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "        r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "\n",
    "        pred_df = predictions.select(\"positionOrder\", \"prediction\").toPandas()\n",
    "        acc_within_1 = np.mean(np.abs(pred_df[\"positionOrder\"] - np.round(pred_df[\"prediction\"])) <= 1) * 100\n",
    "        acc_within_3 = np.mean(np.abs(pred_df[\"positionOrder\"] - np.round(pred_df[\"prediction\"])) <= 3) * 100\n",
    "\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"accuracy_within_1\", acc_within_1)\n",
    "        mlflow.log_metric(\"accuracy_within_3\", acc_within_3)\n",
    "\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': trained_model.featureImportances.toArray()\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"feature_importance.png\")\n",
    "        plt.close()\n",
    "\n",
    "        mlflow.log_artifact(\"feature_importance.png\")\n",
    "        feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
    "        mlflow.log_artifact(\"feature_importance.csv\")\n",
    "        \n",
    "        mlflow.spark.log_model(trained_model, f\"{model_type}_model\")\n",
    "        \n",
    "        return trained_model, rmse, r2, acc_within_1, acc_within_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a556df-c5cb-4b59-a9b1-7356c0c39d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "rf_params = [\n",
    "    {'numTrees': 100, 'maxDepth': 10, 'seed': 42},\n",
    "    {'numTrees': 200, 'maxDepth': 15, 'seed': 42},\n",
    "    {'numTrees': 300, 'maxDepth': 5, 'seed': 42},\n",
    "    {'numTrees': 100, 'maxDepth': 20, 'seed': 42},\n",
    "    {'numTrees': 200, 'maxDepth': 10, 'seed': 42}\n",
    "]\n",
    "\n",
    "gbt_params = [\n",
    "    {'maxIter': 100, 'stepSize': 0.1, 'maxDepth': 3, 'seed': 42},\n",
    "    {'maxIter': 200, 'stepSize': 0.05, 'maxDepth': 5, 'seed': 42},\n",
    "    {'maxIter': 100, 'stepSize': 0.01, 'maxDepth': 7, 'seed': 42},\n",
    "    {'maxIter': 200, 'stepSize': 0.1, 'maxDepth': 5, 'seed': 42},\n",
    "    {'maxIter': 300, 'stepSize': 0.05, 'maxDepth': 3, 'seed': 42}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Random Forest experiments\n",
    "for i, params in enumerate(rf_params):\n",
    "    print(f\"RF Experiment {i+1}/{len(rf_params)}\")\n",
    "    _, rmse, r2, acc1, acc3 = train_model_simple('rf', params)\n",
    "    results.append({\n",
    "        'Model Type': 'Random Forest',\n",
    "        'Parameters': params,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Accuracy within ±1': acc1,\n",
    "        'Accuracy within ±3': acc3\n",
    "    })\n",
    "\n",
    "# Gradient Boosting experiments\n",
    "for i, params in enumerate(gbt_params):\n",
    "    print(f\"GBT Experiment {i+1}/{len(gbt_params)}\")\n",
    "    _, rmse, r2, acc1, acc3 = train_model_simple('gbt', params)\n",
    "    results.append({\n",
    "        'Model Type': 'Gradient Boosting',\n",
    "        'Parameters': params,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Accuracy within ±1': acc1,\n",
    "        'Accuracy within ±3': acc3\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_model = results_df.loc[results_df['RMSE'].idxmin()]\n",
    "\n",
    "print(\"\\n===== Experiment Results =====\")\n",
    "print(results_df[['Model Type', 'RMSE', 'R²', 'Accuracy within ±1', 'Accuracy within ±3']])\n",
    "print(\"\\n===== Best Model =====\")\n",
    "print(f\"Model Type: {best_model['Model Type']}\")\n",
    "print(f\"Parameters: {best_model['Parameters']}\")\n",
    "print(f\"RMSE: {best_model['RMSE']:.4f}\")\n",
    "print(f\"R²: {best_model['R²']:.4f}\")\n",
    "print(f\"Accuracy within ±1 position: {best_model['Accuracy within ±1']:.2f}%\")\n",
    "print(f\"Accuracy within ±3 positions: {best_model['Accuracy within ±3']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "093a0e09-dfaf-453f-8558-e4b33de04499",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Best Model**\n",
    "\n",
    "For this F1 position prediction project, we testes 10 different models: 5 Random Forest and 5 Gradient Boosting models with different hyperparameters. The best performing model was a Gradient Boosting Regressor with 300 trees, a learning rate of 0.05, and a maximum depth of 3. This model had an RMSE of 4.24 and could explain about 69% of what determines a driver's finishing position. In simpler terms, our model could predict a driver's finishing position within 1 spot of their actual position 30% of the time, and within 3 spots 64% of the time. This beat all our Random Forest models. We think the model worked well because we didn't make the trees too deep or the learning too aggressive, which helped it avoid memorizing the training data instead of learning real patterns."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "take-home-exercise-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
